{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2df537",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade librosa\n",
    "%pip install --upgrade numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c345b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import obspy\n",
    "import sys\n",
    "import os\n",
    "from scipy import stats,signal\n",
    "import librosa\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "# import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4ee556",
   "metadata": {},
   "source": [
    "# Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('data')\n",
    "orca_df = pd.read_csv(\"orca_data.csv\")\n",
    "noise_df = pd.read_csv(\"noise_data.csv\")\n",
    "\n",
    "orca_df = orca_df.sample(n=len(orca_df)) # shape = (97, 3) whole // 20\n",
    "noise_df = noise_df.sample(n=len(noise_df)) # shape = (107, 3) whole // 3\n",
    "print(orca_df.shape)\n",
    "print(noise_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0299865",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff5bd57",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "416ce78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('orca_spectrogram_2')\n",
    "# os.makedirs('noise_spectrogram_2')\n",
    "\n",
    "# Add column with labels to orca_df and noise_df, then merge the two together\n",
    "orca_df['label'] = 1\n",
    "noise_df['label'] = 0\n",
    "data_df = pd.concat([orca_df, noise_df], ignore_index=True)\n",
    "num_total_data = len(data_df) # 1 data = 1 row in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e9ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do not hardcode the path\n",
    "dir_path = ''\n",
    "\n",
    "# Get the local work directory\n",
    "\n",
    "for index, row in tqdm(data_df.iterrows(), total=len(data_df)):\n",
    "    filename = row['wav_filename']\n",
    "    filepath = 'wav/' + filename\n",
    "    start_time = row['start_time_s']  # Keep in seconds\n",
    "    end_time = row['end_time_s']  # Keep in seconds\n",
    "\n",
    "    # Load the original wav file\n",
    "    # Load only the necessary chunk using offset and duration\n",
    "    duration = end_time - start_time\n",
    "    y, sr = librosa.load(filepath, sr=None, offset=start_time, duration=duration)\n",
    "    \n",
    "    # Generate a Short-Time Fourier Transform (STFT) spectrogram\n",
    "    n_fft = 2048  # Window size (number of samples) - same as default\n",
    "    hop_length = n_fft // 4  # Hop length - default is usually n_fft // 4\n",
    "    window_type = 'hann'  # Window type - same as default\n",
    "    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length, window=window_type)\n",
    "\n",
    "    # Convert amplitude spectrogram to dB-scaled spectrogram\n",
    "    spectrogram = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "\n",
    "    # Save the spectrogram as a NumPy array\n",
    "    dir_name = ''\n",
    "    if row['label'] == 1:\n",
    "        dir_name = 'orca_spectrogram_2'\n",
    "        spectro_path = os.path.join(dir_path, dir_name)\n",
    "        os.makedirs(spectro_path, exist_ok=True)\n",
    "    else:\n",
    "        dir_name = 'noise_spectrogram_2'\n",
    "        noise_path = os.path.join(dir_path, dir_name)\n",
    "        os.makedirs(noise_path, exist_ok=True)\n",
    "    np.save(os.path.join(dir_path, dir_name, str(index) + '.npy'), spectrogram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b5e9a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation functions\n",
    "\n",
    "# Shift the spectrogram along the time axis\n",
    "def time_shift(spectrogram, shift): # shift = 10-20 % of spectrogram width\n",
    "    return np.roll(spectrogram, shift, axis=1)\n",
    "\n",
    "# Randomly mask certain frequency bands in the spectrogram\n",
    "def frequency_mask(spectrogram, mask_percentage=0.1):\n",
    "    num_freqs = spectrogram.shape[0]\n",
    "    mask_size = int(num_freqs * mask_percentage)\n",
    "    start = np.random.randint(0, num_freqs - mask_size)\n",
    "    spectrogram[start:start+mask_size, :] = 0\n",
    "    return spectrogram\n",
    "\n",
    "#  Randomly mask certain time frames in the spectrogram\n",
    "def time_mask(spectrogram, mask_percentage=0.1):\n",
    "    num_time_frames = spectrogram.shape[1]\n",
    "    mask_size = int(num_time_frames * mask_percentage)\n",
    "    start = np.random.randint(0, num_time_frames - mask_size)\n",
    "    spectrogram[:, start:start+mask_size] = 0\n",
    "    return spectrogram\n",
    "\n",
    "# Inverts the spectrogram by flipping the sign of each element.\n",
    "def flip_signs(spectrogram):\n",
    "    return -spectrogram\n",
    "\n",
    "# Linearly combines two spectrograms with a specified weight\n",
    "def linear_combo(spectrogram1, spectrogram2, alpha=0.5):\n",
    "    return alpha * spectrogram1 + (1 - alpha) * spectrogram2\n",
    "\n",
    "# Scales the entire spectrogram by a random factor to simulate different noise intensities\n",
    "def random_scaling(spectrogram, scale_range=(0.8, 1.2)):\n",
    "    scale = np.random.uniform(scale_range[0], scale_range[1])\n",
    "    return spectrogram * scale\n",
    "\n",
    "# Adds a small random offset to all elements of the spectrogram\n",
    "def add_random_offset(spectrogram, offset_range=(-0.1, 0.1)):\n",
    "    offset = np.random.uniform(offset_range[0], offset_range[1])\n",
    "    return spectrogram + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c26fbb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUGMENTING NOISE DATA: 17934 - 3887 = 14047 more samples needed\n",
    "# Try getting 14047 / 6 = 2341 by each method.\n",
    "\n",
    "# Load all files so random samples can be loaded later\n",
    "all_files = os.listdir('orca_spectrogram_2/')\n",
    "\n",
    "\n",
    "# Randomly create 6 groups of original noise samples\n",
    "num_data_groups = 6\n",
    "num_data_per_group = 2341\n",
    "aug_files = [];\n",
    "for i in range(num_data_groups):\n",
    "    random_files = random.sample(all_files, num_data_per_group)\n",
    "    aug_files.append(random_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef31715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_orca_data_list = []\n",
    "\n",
    "for i in range(0, num_data_groups):\n",
    "    for file_name in aug_files[i]:\n",
    "        original_spect = np.load(os.path.join('orca_spectrogram_2', file_name))\n",
    "        if i == 0:\n",
    "            new_spect = time_shift(original_spect, 6)\n",
    "        elif i == 1:\n",
    "            new_spect = frequency_mask(original_spect)\n",
    "        elif i == 2:\n",
    "            new_spect = time_mask(original_spect)\n",
    "        elif i == 3:\n",
    "            new_spect = flip_signs(original_spect)\n",
    "        elif i == 4:\n",
    "            new_spect = random_scaling(original_spect)\n",
    "        else:\n",
    "            new_spect = add_random_offset(original_spect)\n",
    "        aug_orca_data_list.append(new_spect)\n",
    "            \n",
    "aug_orca_data = np.zeros(shape=(len(aug_orca_data_list), 1, 1025, 44))\n",
    "for i in range(len(aug_orca_data_list)):\n",
    "    aug_orca_data[i] = aug_orca_data_list[i]\n",
    "aug_labels = np.ones(shape=num_data_groups*num_data_per_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2448e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start=-50\n",
    "input_window_length=100 # in seconds\n",
    "fs=22050 # target sampling rate\n",
    "\n",
    "# number_data_per_class=100 # number of data samples per class\n",
    "# num_channels=1  # number of components to check\n",
    "\n",
    "# SNR_THR = 0\n",
    "\n",
    "# all_data=False\n",
    "shifting=True\n",
    "augmentation=False\n",
    "\n",
    "\n",
    "# training parameters\n",
    "train_split = 80\n",
    "val_split=10\n",
    "test_split = 10\n",
    "learning_rate=0.01\n",
    "batch_size=128\n",
    "n_epochs=100\n",
    "dropout=0.4\n",
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4247f23",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52cc8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrcaCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2, num_channels=1, dropout_rate=0.4):\n",
    "        super(OrcaCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 253 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # Dropout layers\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1_bn = nn.BatchNorm1d(128)\n",
    "        self.fc2_bn = nn.BatchNorm1d(num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.dropout1(x)  # Dropout after first convolutional block\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.dropout2(x)  # Dropout after second convolutional block\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor before fully connected layers\n",
    "        x = F.relu(self.fc1_bn(self.fc1(x)))\n",
    "        x = self.dropout3(x)  # Dropout after first fully connected layer\n",
    "        x = self.fc2_bn(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9079b0e5",
   "metadata": {},
   "source": [
    "# Testing CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c251f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the input shape is (batch_size, num_channels, num_features) \n",
    "num_channels = 1\n",
    "num_features = 5000\n",
    "# Create a random input tensor with the specified shape\n",
    "random_input = torch.randn(batch_size, num_channels,int(input_window_length*fs)).to(device)\n",
    "# Initialize your model\n",
    "model = OrcaCNN(num_classes=2, num_channels=num_channels).to(device)  # Use 'cuda' if you have a GPU available\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters in OrcaCNN in log10: {np.log10(total_params)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af40c36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class OrcaDataSet(Dataset): # create custom dataset\n",
    "    def __init__(self, data, labels, num_classes): # initialize\n",
    "        self.data = data \n",
    "        self.labels = labels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample_data = self.data[index]\n",
    "        sample_labels = self.labels[index]\n",
    "        sample_data = torch.Tensor(sample_data)\n",
    "        sample_labels = torch.tensor(sample_labels).float()\n",
    "        return sample_data, sample_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b90b0d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, test_loader, n_epochs=100,\n",
    "                 learning_rate=0.01, criterion=nn.CrossEntropyLoss(),\n",
    "                 augmentation=False, patience=10):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Save loss and error for plotting\n",
    "    loss_time = np.zeros(n_epochs)\n",
    "    val_loss_time = np.zeros(n_epochs)\n",
    "    val_accuracy_time = np.zeros(n_epochs)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    total = 0   # to store the total number of samples\n",
    "    correct = 0 # to store the number of correct predictions\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.long()  # Convert labels to long\n",
    "            \n",
    "            \n",
    "#             # Data augmentation.\n",
    "#             if augmentation:\n",
    "#                 inoise = torch.where(labels == 0)[0] # this finds noise labels in the entire data\n",
    "#                 # we will add noise to the data randomly\n",
    "#                 for iibatch in range(inputs.shape[0]):\n",
    "#                     if np.random.rand(1)>0.5:\n",
    "#                         iik = inoise[torch.randperm(len(inoise))][0]\n",
    "#                         noise = shuffle_phase_tensor(inputs[iik,:,:]).to(device)\n",
    "#                         inputs[iibatch,:,:] = inputs[iibatch,:,:] + torch.rand(1).to(device)*noise/2\n",
    "            \n",
    "\n",
    "            # Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # computing the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # updating the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # updating the training loss list\n",
    "        loss_time[epoch] = running_loss/len(train_loader)\n",
    "\n",
    "        # We evaluate the model, so we do not need the gradient\n",
    "        model.eval()\n",
    "        with torch.no_grad(): # Context-manager that disabled gradient calculation.\n",
    "            # Loop on samples in test set\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            running_test_loss = 0\n",
    "            for data in val_loader:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.long()  # Ensure labels are long\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                running_test_loss += criterion(outputs, labels).item()\n",
    "\n",
    "                correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "    # Check for improvement\n",
    "            if running_test_loss/len(val_loader) < best_val_loss:\n",
    "                best_val_loss = running_test_loss/len(val_loader)\n",
    "                epochs_no_improve = 0\n",
    "                # Save the model if you want to keep the best one\n",
    "                # torch.save(model.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                # print(f'No improvement in validation loss for {epochs_no_improve} epochs.')\n",
    "\n",
    "            if epochs_no_improve == patience:\n",
    "                # print('Early stopping triggered.')\n",
    "                break\n",
    "\n",
    "        \n",
    "        val_loss_time[epoch] = running_test_loss/len(val_loader)\n",
    "\n",
    "        val_accuracy_time[epoch] = 100 * correct / total\n",
    "        # Print intermediate results on screen\n",
    "#         if (epoch+1) % 10 == 0:\n",
    "        if val_loader is not None:\n",
    "            print('[Epoch %d] loss: %.3f - accuracy: %.3f' %\n",
    "            (epoch + 1, running_loss/len(train_loader), 100 * correct / total))\n",
    "        else:\n",
    "            print('[Epoch %d] loss: %.3f' %\n",
    "            (epoch + 1, running_loss/len(train_loader)))\n",
    "\n",
    "\n",
    "    # Optionally, load the best model saved\n",
    "    # model.load_state_dict(torch.load('best_model.pth'))\n",
    "    # testing\n",
    "    model.eval() \n",
    "    with torch.no_grad(): # Context-manager that disabled gradient calculation.\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        running_test_loss = 0\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.long()  # Ensure labels are long\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            running_test_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        test_loss = running_test_loss/len(test_loader)\n",
    "        test_accuracy = 100 * correct / total  \n",
    "        print('test loss: %.3f and accuracy: %.3f' % ( test_loss, test_accuracy))\n",
    "        \n",
    "    return loss_time, val_loss_time, val_accuracy_time, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efcefa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4b7bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_spect(spect):\n",
    "    mean = np.mean(spectrogram)\n",
    "    std_dev = np.std(spectrogram)\n",
    "    normalized_spectrogram = (spectrogram - mean) / std_dev\n",
    "    return normalized_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09128844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading first sepctrogram, shape = ((1025, 44))\n",
    "spectrogram_1 = np.load(os.path.join(dir_path, \"orca_spectrogram\", \"1.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e657c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = spectrogram_1.shape[0]\n",
    "data = np.zeros(shape=(num_total_data, 1, N, spectrogram_1.shape[1]))\n",
    "labels = np.zeros(num_total_data)\n",
    "print(spectrogram_1.shape)\n",
    "\n",
    "# Shuffling data_df\n",
    "random.seed(1234)\n",
    "data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(dir_path, 'noise_spectrogram', '206.npy')\n",
    "test_spect = np.load(file_path)\n",
    "test_spect = test_spect[:N, :]\n",
    "\n",
    "print(test_spect.shape)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.imshow(test_spect, origin='lower')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel spectrogram  Cesptrum')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populating data and labels arrays\n",
    "non_existant_indices = []\n",
    "for index, row in data_df.iterrows():\n",
    "    file_name = str(index) + \".npy\"\n",
    "    \n",
    "    cat_path = \"\"\n",
    "    if row['label'] == 0:\n",
    "        cat_path = 'noise_spectrogram_2'\n",
    "    else:\n",
    "        cat_path = 'orca_spectrogram_2'\n",
    "    \n",
    "    file_path = os.path.join(dir_path, cat_path, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        spectrogram = np.load(file_path)\n",
    "        \n",
    "        if spectrogram.shape[1] > 44:\n",
    "            print(row)\n",
    "\n",
    "        # Normalize using std deviation\n",
    "        spectrogram = normalize_spect(spectrogram)\n",
    "\n",
    "        # Add spectrogram to data array\n",
    "        data[index] = spectrogram\n",
    "        \n",
    "\n",
    "        # Populate labels array\n",
    "        labels[index] = row['label'] \n",
    "    else:\n",
    "        non_existant_indices.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d858cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "print(len(labels))\n",
    "data = np.concatenate((data, aug_orca_data))\n",
    "labels = np.concatenate((labels, aug_labels))\n",
    "\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "data_tensor = torch.tensor(data)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391f3e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tensor = labels_tensor.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b433ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.imshow(data[1][0], origin='lower')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel spectrogram  Cesptrum')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b7a30",
   "metadata": {},
   "source": [
    "## Shuffle and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96798a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the data a OrcaDataSet\n",
    "custom_dataset = OrcaDataSet(data_tensor, labels_tensor, 2)\n",
    "# first split train+val\n",
    "# Determine the size of the training set\n",
    "train_size = int(train_split/100 * len(custom_dataset)) # 80% of the data set\n",
    "val_size = int(val_split/100 * len(custom_dataset)) # 10% of the data set\n",
    "test_size = len(custom_dataset) - train_size - val_size # the rest is test\n",
    "train_dataset, val_dataset = random_split(custom_dataset, [train_size, test_size+val_size])\n",
    "# then split val into val+test\n",
    "test_dataset, val_dataset = random_split(val_dataset, [test_size,val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d366d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader),len(val_loader),len(test_loader))\n",
    "#print(data_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32ec934",
   "metadata": {},
   "source": [
    "## Training and Testing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f232f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expects: input[8, 1, 1025] [batch_size, num_channels, sequence_length]\n",
    "\n",
    "model = OrcaCNN(num_classes=2, num_channels=num_channels,dropout_rate=dropout).to(device)\n",
    "\n",
    "loss_time, val_loss_time, val_accuracy_time, test_loss, test_accuracy = train_model(model,\n",
    "    train_loader,val_loader,test_loader,n_epochs=n_epochs,learning_rate=learning_rate,\n",
    "    criterion=criterion,augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91912e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = np.count_nonzero(loss_time)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Training Loss')\n",
    "ax1.plot(np.arange(1, NN+1), loss_time[:NN], color='tab:red')\n",
    "ax1.plot(np.arange(1, NN+1), val_loss_time[:NN], color='tab:blue')\n",
    "ax1.plot(NN+1, test_loss, 'p', color='tab:blue')\n",
    "# ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Accuracy', color='tab:green')\n",
    "ax2.plot(np.arange(1, NN+1), val_accuracy_time[:NN], color='tab:green')\n",
    "ax2.plot(NN+1, test_accuracy, 's', color='tab:green')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:green')\n",
    "plt.title(\"Training and Validation Loss and Accuracy on OrcaCNN\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig(\"OrcaCNN.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
